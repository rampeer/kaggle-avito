{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "from tqdm import *\n",
    "import zipfile\n",
    "from PIL import Image\n",
    "import keras\n",
    "import pickle\n",
    "# from keras import backend as K\n",
    "from __future__ import division, print_function\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from random import shuffle\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/grozin/.kaggle/competitions/avito-demand-prediction/\"\n",
    "train_image_base_path = base_path + \"data/competition_files/train_jpg/\"\n",
    "test_image_base_path = base_path + \"data/competition_files/test_jpg/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "training = pd.read_csv(base_path + 'train.csv', index_col = \"item_id\", parse_dates = [\"activation_date\"])\n",
    "testing = pd.read_csv(base_path + 'test.csv', index_col = \"item_id\", parse_dates = [\"activation_date\"])\n",
    "testing_imgs = testing[\"image\"][~testing[\"image\"].isnull()]\n",
    "training_imgs = training[\"image\"][~training[\"image\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import itemfreq\n",
    "import operator\n",
    "import cv2\n",
    "from skimage import feature\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "train_images = listdir(train_image_base_path)\n",
    "test_images = listdir(test_image_base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_analysis(img):\n",
    "    # obtain the color palatte of the image \n",
    "    palette = defaultdict(int)\n",
    "    for pixel in img.getdata():\n",
    "        palette[pixel] += 1\n",
    "    \n",
    "    # sort the colors present in the image \n",
    "    sorted_x = sorted(palette.items(), key=operator.itemgetter(1), reverse = True)\n",
    "    light_shade, dark_shade, shade_count, pixel_limit = 0, 0, 1, 25\n",
    "    for i, x in enumerate(sorted_x[:pixel_limit]):\n",
    "        if all(xx <= 20 for xx in x[0][:3]): ## dull : too much darkness \n",
    "            dark_shade += x[1]\n",
    "        if all(xx >= 240 for xx in x[0][:3]): ## bright : too much whiteness \n",
    "            light_shade += x[1]\n",
    "        shade_count += x[1]\n",
    "        \n",
    "    light_percent = round((float(light_shade)/shade_count)*100, 2)\n",
    "    dark_percent = round((float(dark_shade)/shade_count)*100, 2)\n",
    "    return light_percent, dark_percent\n",
    "\n",
    "def color_analysis2(img):\n",
    "    return np.sum(np.where(np.all(img < 20, axis=2))) / img.size, np.sum(np.where(np.all(img > 240, axis=2))) / img.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_pixel_width(img):\n",
    "    im_array = np.asarray(img.convert(mode='L'))\n",
    "    edges_sigma1 = feature.canny(im_array, sigma=3)\n",
    "    apw = (float(np.sum(edges_sigma1)) / (img.size[0]*img.size[1]))\n",
    "    return apw*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_f = image.load_img(train_image_base_path + train_images[123])\n",
    "target_size = (224, 224)\n",
    "img_rescaled = img_f.resize(target_size)\n",
    "img_rescaled = image.img_to_array(img_rescaled)\n",
    "\n",
    "dom_c = get_dominant_color(cv2.cvtColor(img_rescaled, cv2.COLOR_RGB2YUV))\n",
    "print(dom_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.array([get_dominant_color(cv2.cvtColor(np.array([[[227.707    ,  -1.8158445,  -1.8740396]]]), cv2.COLOR_YUV2RGB))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.cvtColor(cv2.cvtColor(img_rescaled[:1, :1, :], cv2.COLOR_RGB2YUV), cv2.COLOR_YUV2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rescaled[:1, :1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_color(img):\n",
    "    average_color = [img[:, :, i].mean() for i in range(img.shape[-1])]\n",
    "    return average_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size(filename):\n",
    "    filename = filename\n",
    "    st = os.stat(filename)\n",
    "    return st.st_size\n",
    "\n",
    "def get_dimensions(img):\n",
    "    img_size = img.size\n",
    "    return img_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blurrness_score(img):\n",
    "    image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    fm = cv2.Laplacian(image, cv2.CV_64F).var()\n",
    "    return fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.applications.inception_v3 as inception_v3\n",
    "\n",
    "def classify_by_model(model, bak, img):\n",
    "    \"\"\"Classify image and return top match.\"\"\"\n",
    "    x = bak.preprocess_input(img)\n",
    "    preds = model.predict(x)\n",
    "    return int(np.argmax(preds)), np.max(preds) # bak.decode_predictions(preds, top=1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_img_features(filename, inception_model):\n",
    "    img_f = image.load_img(filename)\n",
    "    img = image.img_to_array(img_f)\n",
    "    \n",
    "    target_size = (224, 224)\n",
    "    img_rescaled = img_f.resize(target_size)\n",
    "    img_rescaled = np.array([image.img_to_array(img_rescaled)])\n",
    "    \n",
    "    return np.array(\n",
    "        [get_blurrness_score(np.array(img, np.uint16)),\n",
    "          get_size(filename), \n",
    "          get_dimensions(img),\n",
    "          img.shape[0], img.shape[1], \n",
    "          *get_average_color(img),\n",
    "          average_pixel_width(img_f),\n",
    "          *color_analysis2(img),\n",
    "          *classify_by_model(inception_model, inception_v3, img_rescaled)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_img_features_names = [\n",
    "    \"burrness\", \n",
    "    \"size\", \"dims\",\n",
    "    \"w\", \"h\", \n",
    "    \"avg_color_r\", \"avg_color_g\", \"avg_color_b\",\n",
    "    \"pixel_w\",\n",
    "    \"light\", \"darkness\", \"img_class\", \"img_proba\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from time import time\n",
    "\n",
    "def process_img_features(worker_id, imgs_full_paths, imgs):\n",
    "    with open(str(worker_id) + \"_img.pickle\", \"wb\") as f:\n",
    "        graph1 = tf.Graph()\n",
    "        with graph1.as_default():\n",
    "            with tf.Session(graph=graph1) as sess:\n",
    "                inception_model = inception_v3.InceptionV3(weights='imagenet')\n",
    "                start_time = time()\n",
    "                for i, (img_full_path, img_tag) in enumerate(zip(imgs_full_paths, imgs)):\n",
    "                    try:\n",
    "                        if i % 150 == 0:\n",
    "                            dt = int(time() - start_time)\n",
    "                            print(worker_id, i, \"/\", len(imgs), \" (time from start: \", dt, \")\")\n",
    "                            speed = float(i + 1) / float(dt + 1.0)\n",
    "                            print((len(imgs) - i) / speed)\n",
    "                        pickle.dump((img_tag, get_simple_img_features(img_full_path, inception_model)), f)\n",
    "                    except:\n",
    "                        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "img_full_paths = []\n",
    "for x in testing_imgs:\n",
    "    imgs.append(x)\n",
    "    img_full_paths.append(test_image_base_path + x + \".jpg\")\n",
    "    \n",
    "for x in training_imgs:\n",
    "    imgs.append(x)\n",
    "    img_full_paths.append(train_image_base_path + x + \".jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgs = imgs[:4000]\n",
    "# img_full_paths = img_full_paths[:4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_count = 8\n",
    "threads = []\n",
    "\n",
    "for i, thread_fp, thread_imgs in zip(range(thread_count),\n",
    "           np.array_split(np.array(img_full_paths), thread_count),\n",
    "           np.array_split(np.array(imgs), thread_count)):\n",
    "    threads.append(threading.Thread(target=process_img_features, args = ([i, thread_fp, thread_imgs])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(thread_count):\n",
    "    threads[i].start()\n",
    "for i in range(thread_count):\n",
    "    threads[i].join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Throwing in few classifier features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_model = inception_v3.InceptionV3(weights='imagenet')\n",
    "vgg_model = vgg19.VGG19(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(train_image_base_path + train_images[5])\n",
    "target_size = (224, 224)\n",
    "if img.size != target_size:\n",
    "    img = img.resize(target_size)\n",
    "img = np.array([image.img_to_array(img)])\n",
    "\n",
    "x = inception_v3.preprocess_input(img)\n",
    "preds = inception_model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "import tensorflow as tf\n",
    "\n",
    "def process_image_class(worker_id, imgs_full_path, imgs):\n",
    "    graph1 = tf.Graph()\n",
    "    with graph1.as_default():\n",
    "        with tf.Session(graph=graph1) as sess:\n",
    "            inception_model = inception_v3.InceptionV3(weights='imagenet')\n",
    "            vgg_model = vgg19.VGG19(weights='imagenet')\n",
    "            with open(str(worker_id) + \"_img_classes.pickle\", \"wb\") as f:\n",
    "                for i, (img_full_path, img_tag) in enumerate(zip(imgs_full_path, imgs)):\n",
    "                    if i % 100 == 0:\n",
    "                        print(worker_id, i)\n",
    "                    try:\n",
    "                        img = Image.open(img_full_path)\n",
    "                        target_size = (224, 224)\n",
    "                        if img.size != target_size:\n",
    "                            img = img.resize(target_size)\n",
    "                        img = np.array([image.img_to_array(img)])\n",
    "                        results = classify_by_model(inception_model, inception_v3, img.copy())\n",
    "                        pickle.dump((img_tag, results), f)\n",
    "                    except:\n",
    "                        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_count = 4\n",
    "threads = []\n",
    "img_full_paths = [(train_image_base_path + x) for x in train_images] + [(test_image_base_path + x) for x in test_images]\n",
    "imgs = train_images + test_images\n",
    "\n",
    "# img_full_paths = img_full_paths[:1000]\n",
    "# imgs = imgs[:1000]\n",
    "print(len(img_full_paths), len(imgs))\n",
    "for i, thread_fp, thread_imgs in zip(range(thread_count),\n",
    "                   np.array_split(np.array(img_full_paths), thread_count),\n",
    "                   np.array_split(np.array(imgs), thread_count)):\n",
    "    threads.append(threading.Thread(target=process_image_class, args = ([i, thread_fp, thread_imgs])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "a = time()\n",
    "for i in range(thread_count):\n",
    "    threads[i].start()\n",
    "for i in range(thread_count):\n",
    "    threads[i].join()\n",
    "time() - a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HSV / YUV features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def get_dominant_color(img):\n",
    "    pixels = img.reshape((-1, 3))\n",
    "\n",
    "    n_colors = 5\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 50, .3)\n",
    "    flags = cv2.KMEANS_RANDOM_CENTERS\n",
    "    _, labels, centroids = cv2.kmeans(pixels, n_colors, None, criteria, 5, flags)\n",
    "\n",
    "    palette = centroids\n",
    "    quantized = palette[labels.flatten()]\n",
    "    quantized = quantized.reshape(img.shape)\n",
    "\n",
    "    counts = np.unique(labels, return_counts=True)[1]\n",
    "    count_indices = np.argsort(-counts)\n",
    "    return np.concatenate([palette[count_indices[0]], palette[count_indices[1]], \\\n",
    "            [counts[count_indices[0]] / sum(counts)], \\\n",
    "            [counts[count_indices[1]] / sum(counts)]])\\\n",
    "\n",
    "\n",
    "def palette_features(filename):\n",
    "    img_f = image.load_img(filename)\n",
    "    img = image.img_to_array(img_f)\n",
    "    \n",
    "    target_size = (224, 224)\n",
    "    img_rescaled = img_f.resize(target_size)\n",
    "    rgb_array = image.img_to_array(img_rescaled)\n",
    "    \n",
    "    yuv_array = cv2.cvtColor(rgb_array, cv2.COLOR_RGB2YUV)\n",
    "    hls_array = cv2.cvtColor(rgb_array, cv2.COLOR_RGB2HLS)\n",
    "\n",
    "    rg = rgb_array[:, :, 0] - rgb_array[:, :, 1]\n",
    "    yb = (rgb_array[:, :, 0] + rgb_array[:, :, 1]) / 2 - rgb_array[:, :, 2]\n",
    "    cful = [np.sqrt(np.var(rg) + np.var(yb)) + 0.3 * np.sqrt(np.mean(rg) ** 2.0 + np.mean(yb) ** 2.0)]\n",
    "\n",
    "    features = np.concatenate([\n",
    "        cful,\n",
    "        get_dominant_color(rgb_array),\n",
    "        np.mean(yuv_array, axis=(0, 1)),\n",
    "        np.var(yuv_array, axis=(0, 1)),\n",
    "        np.mean(hls_array, axis=(0, 1)),\n",
    "        np.var(hls_array, axis=(0, 1))\n",
    "    ])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pal_feature_names = [\n",
    "    \"cful\",\n",
    "    \"dom1y\", \"dom1u\", \"dom1v\",\n",
    "    \"dom2y\", \"dom2u\", \"dom2v\",\n",
    "    \"dom1portion\", \"dom2portion\",\n",
    "    \"ymean\", \"umean\", \"vmean\",\n",
    "    \"yvar\", \"uvar\", \"vvar\",\n",
    "    \"hmean\", \"lmean\", \"smean\",\n",
    "    \"hvar\", \"lvar\", \"svar\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import threading\n",
    "from time import time\n",
    "\n",
    "def process_img_pal_features(worker_id, imgs_full_paths, imgs):\n",
    "    with open(str(worker_id) + \"_pal.pickle\", \"wb\") as f:\n",
    "        graph1 = tf.Graph()\n",
    "        with graph1.as_default():\n",
    "            with tf.Session(graph=graph1) as sess:\n",
    "                start_time = time()\n",
    "                for i, (img_full_path, img_tag) in enumerate(zip(imgs_full_paths, imgs)):\n",
    "                    try:\n",
    "                        if i % 150 == 0:\n",
    "                            dt = int(time() - start_time)\n",
    "                            print(worker_id, i, \"/\", len(imgs), \" (time from start: \", dt, \")\")\n",
    "                            speed = float(i + 1) / float(dt + 1.0)\n",
    "                            print((len(imgs) - i) / speed)\n",
    "                        pickle.dump((img_tag, palette_features(img_full_path)), f)\n",
    "                    except:\n",
    "                        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thread_count = 16\n",
    "threads = []\n",
    "img_full_paths = [(train_image_base_path + x) for x in train_images] + [(test_image_base_path + x) for x in test_images]\n",
    "imgs = train_images + test_images\n",
    "\n",
    "# img_full_paths = img_full_paths[:1000]\n",
    "# imgs = imgs[:1000]\n",
    "\n",
    "print(len(img_full_paths), len(imgs))\n",
    "for i, thread_fp, thread_imgs in zip(range(thread_count),\n",
    "                   np.array_split(np.array(img_full_paths), thread_count),\n",
    "                   np.array_split(np.array(imgs), thread_count)):\n",
    "    threads.append(threading.Thread(target=process_img_pal_features, args = ([i, thread_fp, thread_imgs])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "a = time()\n",
    "for i in range(thread_count):\n",
    "    threads[i].start()\n",
    "for i in range(thread_count):\n",
    "    threads[i].join()\n",
    "time() - a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segment features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyimgsaliency import get_saliency_ft, get_saliency_rbd, get_saliency_mbd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "feat5 = cv2.FastFeatureDetector_create(5)\n",
    "feat10 = cv2.FastFeatureDetector_create(10)\n",
    "feat20 = cv2.FastFeatureDetector_create(20)\n",
    "\n",
    "cont_thresholds = [64, 128, 192]\n",
    "area_thresholds = [5, 100, 500]\n",
    "def get_saliency_features(filename):\n",
    "    img_f = image.load_img(filename)\n",
    "    img = image.img_to_array(img_f)\n",
    "    \n",
    "    target_size = (128, 128)\n",
    "    img_rescaled = img_f.resize(target_size)\n",
    "    rgb_array = image.img_to_array(img_rescaled)\n",
    "    \n",
    "    kp5 = feat5.detect(rgb_array, None)\n",
    "    kp10 = feat10.detect(rgb_array, None)\n",
    "    kp20 = feat20.detect(rgb_array, None)\n",
    "    \n",
    "    img = cv2.cvtColor(img.astype('uint8'), cv2.COLOR_RGB2GRAY)\n",
    "    contour_features = []\n",
    "    \n",
    "    for thresh in cont_thresholds:        \n",
    "        _, ret = cv2.threshold(img, thresh, 255, 0)\n",
    "        im2, contours, _ = cv2.findContours(ret, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        areas = np.array([cv2.contourArea(x) for x in contours])\n",
    "        contour_features.append(np.sum(ret))\n",
    "        contour_features.append(len(contours))\n",
    "        if len(areas) == 0:\n",
    "            contour_features.append(-1)\n",
    "        else:\n",
    "            contour_features.append(max(areas))\n",
    "        for a in area_thresholds:\n",
    "            contour_features.append(sum(areas > a))\n",
    "        \n",
    "    return [len(kp5), len(kp10), len(kp20)] + contour_features\n",
    "\n",
    "feature_names = [\"kp5\", \"kp10\", \"kp20\"]\n",
    "for thresh in cont_thresholds:        \n",
    "    feature_names.append(\"mask_size_\" + str(thresh))\n",
    "    feature_names.append(\"contour_size_\" + str(thresh))\n",
    "    feature_names.append(\"largest_contour_size_\" + str(thresh))\n",
    "    for a in area_thresholds:\n",
    "        feature_names.append(\"contours_\" + str(thresh) + \"_larger_\" + str(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▋   | 924050/1390836 [7:31:30<3:48:04, 34.11it/s]"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"contour_map.pickle\", \"wb\") as f:\n",
    "    for x in tqdm(test_images):\n",
    "        try:\n",
    "            features = get_saliency_features(test_image_base_path + x)\n",
    "            pickle.dump((x, features), f)\n",
    "        except:\n",
    "            traceback.print_exc()\n",
    "    for x in tqdm(train_images):\n",
    "        try:\n",
    "            features = get_saliency_features(train_image_base_path + x)\n",
    "            pickle.dump((x, features), f)\n",
    "        except:\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0a7c18b1d0>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAFeJJREFUeJzt3X/MXFWdx/H3d1uhWwyWImlKS7bdWNwg4Yd5wo+w2RCq4cey4CaE4BKt2k2zCbugMdFW/mA38Q+IRmQTF/cJILghFLayS8O6PkKFmP1D9Kk2iGCx8kNaCq0uoJENUPzuH/c+dM70zsyd+/Pcmc8rafrMzJ25Z87MfM/3nHvvOebuiIgs+KO2CyAicVFQEJGAgoKIBBQURCSgoCAiAQUFEQkoKIhIoLagYGYXmdluM9tjZpvr2o+IVMvqOHnJzBYBTwMfBvYCPwI+6u5PVr4zEanU4ppe9yxgj7s/A2BmW4HLgcygcJQd7Us4pqaiVOPk014H4OnHl7ZcEpFifscrv3b3E0ZtV1dQWAW80HN7L3B27wZmtgnYBLCEpZxt62sqSjXm5nYBcOGJZ7RcEpFiHvZtz+fZrq6gMJK7zwKzAMfa8sJ9mLkXm/mxKhjItKgrKOwDTuq5vTq9rxILgQD0YxWpWl1HH34ErDOztWZ2FHAVsL2mfYlIhWrJFNz9kJn9PTAHLALucPeflX3d3gyh/z5lDCLVqG1Mwd2/DXy7rtcXkXq0NtDYlqxso5+yDplmOs1ZRAKdyhTKtuB5soQ82ymTkEnWqaBQVpEfc95AIjIp1H0QkcBUZQpFqKsg00aZgogEFBREJKCgICIBBQURCSgoiEhAQUFEAgoKIhJQUBCRgIKCiAQUFEQkoKAgIgEFBREJKCiISEBBQUQCCgoiElBQEJFAFJOsnHza6++s1dgGTaQicpgyBREJRJEptK2OyVmVfUhXFQ4KZnYS8E1gBeDArLvfYmbLgXuBNcBzwJXu/kr5onbL3Iu7FBikMYMatiLfwTLdh0PAZ939FOAc4BozOwXYDOxw93XAjvS2iHRE4UzB3fcD+9O/f2dmTwGrgMuB89PN7gIeBT5fqpQdpCxB6tKfFVT9XatkoNHM1gBnAo8BK9KAAfASSfci6zmbzGzezOYP/ubtKoohIhUoHRTM7N3At4BPu/tvex9zdycZbziCu8+6+4y7z5xw/KKyxRCZClljB1UPlJc6+mBm7yIJCHe7+/3p3S+b2Up3329mK4EDZQspIoNF030wMwNuB55y96/0PLQd2JD+vQF4oHjxRKRpZTKF84CPAT81s4X85QvAjcB9ZrYReB64slwRRbopK60v26o3MYBd5ujD/wA24OH1RV9XRNqlMxpFCho1wNfVw9K69kFEAsoUZOoVPaTX1UxgFAUFKW3hRxXzj2TYoF9vubvwXuqm7oOIBJQpVGyaW5i2DesG5P1c9PkpUxCRPsoUpLQqWtdx+vKDxgeaauV79z+JmYWCgkSpiq6AFKPug4gElCnIWOo+ZKdDgu1TpiAiAWUKMpZxW/BxxwaaHrSUIykoSGnD5gxs4/wABYNy1H0QkUAUmcLTjy/lwhPPqHyuuboHwyTRX89F0ve6ZyiW/JQpiEggikxhQdbValmPtz25hVqxfMY58091Gg9lCiISiCpT6DWs5VCrErdRn8+kHDLsevkHiTYoxGrSL4YpszBu3sHCrIHJSazLrlL3QUQCE5Mp1DHHfpbeQ6cxHppss8XN2ndWVyHGepPDlCmISKB0pmBmi4B5YJ+7X2pma4GtwPHATuBj7v5m2f0MMu5JL5PYSsV8inD/YeTeyVBiHHDUSVTVdB+uA54Cjk1v3wTc7O5bzezrwEbg1gr2k6nMWXNd16UvbFe6D3nPhZlkpboPZrYa+EvgtvS2ARcA29JN7gI+UmYfItKssmMKXwU+B/whvX088Kq7H0pv7wVWldyHDDD34q4jWrSs+2LTP59iFeVt8j33d4Fir+9xlVmK/lLggLvvLPj8TWY2b2bzb/FG0WKISMXKLkV/mZldAiwhGVO4BVhmZovTbGE1sC/rye4+C8wCHGvLvUQ5pkqXxhGaVNcAaV5VDpq2PQBbOFNw9y3uvtrd1wBXAd9z96uBR4Ar0s02AA+ULqWINKaOk5c+D2w1sy8CPwFur2EfMkCXMomsfvmguRmyHptUbb/PSoKCuz8KPJr+/QxwVhWvW5W2B4LKzms47EzBMvtpW28gyBow7YI66ryz3QcRmUzm3v4Y37G23M+29bW9/ritTpUzFte1z6z9dy1TWFDValBN10PW1Z1Vdneqvnr0Yd+2091nRm2nTEFEAhNzlWRRgyJxU33avC3BsDGEac8QyjynKnWcgNU71tLke5vaoNDGufhVX6zVpYt3mpxXs6kf0qgfbd5y9G/X9qX56j6ISGDqMoVhA0N17ytLlfvvyvLtkzLbdu9A4KDWPo9hrzHsvdSVESlTEJHAVGUKdaxCNWxfC2I4Eaftk53ytHh5t2s76ykz9jPO2MM4g9BV1slUnKewIIYfZwza/lH16+pszqMC7bhBLG/3oWiA13kKIlLIVHQfJj1DKLOcXtHWrPc1y6b1XcsSin6fxmn5Qdc+iEgkpiJT6KpxW4q6rtkYtw8by4Bg3UYNXOcZOMw6rDlMEwPGyhREJDDRmULXxxLaanHL9nEnNUMoe3LYqFa+aGZY9dGbiQ4KXRDLDyjPuQKDvtSDzhKN5b1VJevahHHe46BzV/LUV9bnozMaRaQRyhRaEkuXoN84h8za0vbJTnVc0VnXc3q3XbQy33OUKYhIQJlCC+pq5crMWdD2NfzTpoo1K/OOQYxLQaHD2rhUOobBwxjKANV0Y8adgKXcPvbk2l7dBxEJKFPogNjOjR8l1nJVrcn3Oe7l5Vn3aaBRRAoplSmY2TLgNuBUwIFPAbuBe4E1wHPAle7+SqlSTomuTKc2St1LvbV9SLIpVV3nMO6YQtnuwy3Ad9z9CjM7ClgKfAHY4e43mtlmYDPJ+pJRqWL0t6iqugMxHCkY9gOd1B9uk7NF9+9z3OcW+Y4U7j6Y2XuAvyBdQNbd33T3V4HLgbvSze4CPlJ0HyLSvDKZwlrgIPANMzsd2AlcB6xw9/3pNi8BK8oVsR5tt7JFlkNryqS28FWJuX6yMoQmBxoXAx8EbnX3M4Hfk3QV3uHJBJCZk0Ca2SYzmzez+bd4o0QxRKRKZYLCXmCvuz+W3t5GEiReNrOVAOn/B7Ke7O6z7j7j7jMfOO3t1ltuSSxMHFL2TLuYW9MuWfgsRtXpoM+syOdQOCi4+0vAC2b2/vSu9cCTwHZgQ3rfBuCBovsQkeaVPfrwD8Dd6ZGHZ4BPkgSa+8xsI/A8cOWoF3n68aWNtyxtHn1oW9H3HktdFTlK09XMpewqY0U+s1JBwd13AVnzyI+1iMPJp73O3Fy5D67srMTTKO+hxK7XVVcDwijDzl7spWsfRKSUibn2YVJbA2h2Cboup9rTpq4usDIFEQlEkSm0MdA46Sa5PpXNJPJkCEXqKYqgkNekzBI8alGQcZ+bxyT9kCblfZRVV7dS3QcRCXQqUyhr1DJfTWmy1R53fYFBz5VssWavZcqjTEFEAp3JFMq0eNOk6CKlC+pe1HbSxPL+qyyHMgURCXQmU8gaaR3UNy/aF27zeogqTvGuyqBMrO0jGHVP81bGJGWvnQkKkO9c72GXmFZ1aWkMmgxcTZxF2XVd/R5lUfdBRAKdyhSGpffDuhF5V+GJWRVlzHv1Y/92VXSrik48Ks1TpiAigU5lCnlkDYZVPQlFV+V9r+PUVzVrHEpMOhkUhs1Yqy/aYHm6X71BtcqAWXRNi659nl0tdy91H0QkYMks7O061pb72TbWDG7vyDPoGGMXoWxLkvc91T1BS5dbxDrEfC7Fw75tp7tnTZ8YUKYgIoGoxhTqOmMuxpmby/Y9B13x2f9eh7Vco+q7ykFF6Y6ogkIReQYdYwoGZY16L+MEm7xnfioYFNPVQUd1H0QkEFWm0LWIWqWyy7SJVEWZgogESmUKZvYZ4G9JVpb+KcmycSuBrcDxJMvTf8zd3yxZzpG6On4Q45Wbbe8/dsPGCrr6PexVOFMws1XAtcCMu58KLAKuAm4Cbnb39wGvABurKKiINKPsmMJi4I/N7C1gKbAfuAD4m/Txu4B/BG4tuZ/cYpmcNa82JnAdpo0sYdT1KrFlLrGVp2qFg4K77zOzLwO/Av4P+C5Jd+FVdz+UbrYXWFW6lGPqT+EmIaUbVxfea540vIl91aFrjVOvMt2H44DLgbXAicAxwEVjPH+Tmc2b2fxbvFG0GCJSsTLdhw8Bz7r7QQAzux84D1hmZovTbGE1sC/rye4+C8wCzJy+xMsuRZ+lC2le0RasjklXqlakjHW16G3MN9mF71+WMockfwWcY2ZLzcyA9cCTwCPAFek2G4AHyhVRRJpUZkzhMTPbBvwYOAT8hKTl/y9gq5l9Mb3v9ioKWkRXTzNdUHWftIorM8d5jRjrPcYJb2Orp1JHH9z9BuCGvrufAc4q87pFKAAMV0W9dLVue7U5lX5X6k9nNIpIIKprH8rImkwk69BkVw8TFVVl69TGYjB5Z5yue7+D9teV1n8cyhREJBBVplCmJer6mELV6pyspoxRZyoOWnOi97lFylHFuhPT8t2KKiiU0fuBTcuHt2DQOpuDtilj2A8zz4+26sAyzDR/J8pQ90FEAlHM5jxz+hL/4dxJQHwXCLWpiuXuYmwhtYRcOzSbs4gUMjFjCjGr+rBg0/sctv9hYwujKAOIkzIFEQkoU4hEF8Y48h4yVgYQ6trh8qkNCk1OvNLGD37YPsssQJNnn1358tetC4E+i7oPIhKILlNoY9qs/n3HrMxkLHUPPmZlX9OcNXR1GkBlCiISiC5TaEMVh9nKtgp5F3odN5NqsqXOeu8aZ2jmis4q96GgkKGJM+7qnsGoqUVmupYaT6KqP1d1H0Qk0JlMoY0JPmD8KNzbjRi0wElTZxuO+zjk78aMS92I7lCmICKB6K6S7Je3ZWmzJRr3EFyRMYtJ6rsrW2hH3qskO9N9GGXY+QZNLvqRZxKScU1SQJD4qfsgIoGJyRR6jZPC17k8mVr4bBp0jJsyBREJjMwUzOwO4FLggLufmt63HLgXWAM8B1zp7q+ka0reAlwCvA58wt1/XKaAXWltu1JOkVHyZAp3cuQS85uBHe6+DtiR3ga4GFiX/tsE3FpNMUWkKSMzBXf/vpmt6bv7cuD89O+7gEeBz6f3f9OT45w/MLNlZrbS3fdXVeCq5V2BSJmATIuiA40ren7oLwEr0r9XAS/0bLc3vS/aoDCIgkD9NOBYvyJnApceaEyzgrHPgDKzTWY2b2bzB3/zdtliiEhFigaFl81sJUD6/4H0/n1A76mJq9P7juDus+4+4+4zJxy/qGAxZBIoK6tPkSysaFDYDmxI/94APNBz/8ctcQ7wWszjCSJypDyHJO8hGVR8r5ntBW4AbgTuM7ONwPPAlenm3yY5HLmH5JDkJ2sos4iMYSETW7Qy3/bRXxAl00WDjvXRsnEiUoiCgogEFBREJKCgIFGZe3GXDlG2TEFBRAIKCiISUFCQKKkb0R4FBREJKCiISEBBQUQCCgoSNY0rNE9BQaKnQcdmKSiISEBBQUQCCgoiElBQkM7Q2EIzFBREJKCgICIBBQXpHHUh6qWgICIBBQURCSgoiEhAQUE6SYcn66OgICIBBQURCeRZNu4O4FLggLufmt73JeCvgDeBXwKfdPdX08e2ABuBt4Fr3X2uprKLRL2cfX/35sITz4i6vAvyZAp3Ahf13fcQcKq7nwY8DWwBMLNTgKuAD6TP+Rcz05LSIh0yMii4+/eB/+2777vufii9+QOSJecBLge2uvsb7v4syUKzZ1VYXpFMXRt0HDZQmncQta7B1pHdhxw+Bdyb/r2KJEgs2JveJyIM7zYU/YHPvbjrndetontSaqDRzK4HDgF3F3juJjObN7P5g795u0wxRKRChTMFM/sEyQDkej+8nv0+oHdN+dXpfUdw91lgFpKl6IuWQyRWWa111uDjMHm2zxrALJMxFMoUzOwi4HPAZe7+es9D24GrzOxoM1sLrAN+WGQfItKOPIck7wHOB95rZnuBG0iONhwNPGRmAD9w979z95+Z2X3AkyTdimvcXX0DmUrjZgWjnt8UO5z5t2fm9CX+w7mTRm8okkNs5wD0pvLjBIrebUdtl6fb8LBv2+nuM6PKqzMaRSSgTEEmVmwZA7R7BqYyBREppIqTl0Qkpxizl37KFEQkoKAgE0sTsRSjoCAiAQUFEQlEcUjSzA4Cvwd+3XZZgPeicvRSOUJdLsefuPsJozaKIigAmNl8nmOoKofKoXLUWw51H0QkoKAgIoGYgsJs2wVIqRwhlSM08eWIZkxBROIQU6YgIhGIIiiY2UVmttvM9pjZ5ob2eZKZPWJmT5rZz8zsuvT+5Wb2kJn9Iv3/uIbKs8jMfmJmD6a315rZY2md3GtmRzVQhmVmts3Mfm5mT5nZuW3Uh5l9Jv1MnjCze8xsSVP1YWZ3mNkBM3ui577MOrDEP6dletzMPlhzOb6UfjaPm9l/mNmynse2pOXYbWYXltl360EhXRfia8DFwCnAR9P1I+p2CPisu58CnANck+53M7DD3dcBO9LbTbgOeKrn9k3Aze7+PuAVkgV26nYL8B13/zPg9LQ8jdaHma0CrgVm0sWHFpGsJdJUfdzJkeucDKqDi0mmHFwHbAJurbkczay34u6t/gPOBeZ6bm8BtrRQjgeADwO7gZXpfSuB3Q3sezXJl+0C4EHASE5MWZxVRzWV4T3As6TjTD33N1ofJEsCvAAsJ7mK90HgwibrA1gDPDGqDoB/BT6atV0d5eh77K+Bu9O/g98MMAecW3S/rWcKHP4SLGh8rQgzWwOcCTwGrHD3/elDLwErGijCV0kmwv1Devt44FU/vOBOE3WyFjgIfCPtxtxmZsfQcH24+z7gy8CvgP3Aa8BOmq+PXoPqoM3v7qeA/66jHDEEhVaZ2buBbwGfdvff9j7mSdit9fCMmS2s07mzzv3ksBj4IHCru59Jctp50FVoqD6OI1lpbC1wInAMR6bRrWmiDkYps95KHjEEhdxrRVTNzN5FEhDudvf707tfNrOV6eMrgQM1F+M84DIzew7YStKFuAVYZmYLk+A0USd7gb3u/lh6extJkGi6Pj4EPOvuB939LeB+kjpquj56DaqDxr+7PeutXJ0GqMrLEUNQ+BGwLh1dPopkwGR73Tu1ZG7624Gn3P0rPQ9tBzakf28gGWuojbtvcffV7r6G5L1/z92vBh4BrmiwHC8BL5jZ+9O71pNM1d9ofZB0G84xs6XpZ7RQjkbro8+gOtgOfDw9CnEO8FpPN6Nyja23Uueg0RgDKpeQjKb+Eri+oX3+OUka+DiwK/13CUl/fgfwC+BhYHmD9XA+8GD695+mH+we4N+BoxvY/xnAfFon/wkc10Z9AP8E/Bx4Avg3kjVGGqkP4B6SsYy3SLKnjYPqgGRA+Gvp9/anJEdM6izHHpKxg4Xv69d7tr8+Lcdu4OIy+9YZjSISiKH7ICIRUVAQkYCCgogEFBREJKCgICIBBQURCSgoiEhAQUFEAv8PlWP19/I2NrMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "# ft = get_saliency_ft(filename).astype('uint8')\n",
    "# get_saliency_rbd('/home/grozin/.kaggle/competitions/avito-demand-prediction/data/competition_files/train_jpg/f1069b755cb5dbb00dd887765197511c2b17e28ee4fd0be0c6808df3d3c6d9b1.jpg').astype('uint8')\n",
    "# get_saliency_ft('/home/grozin/.kaggle/competitions/avito-demand-prediction/data/competition_files/train_jpg/f1069b755cb5dbb00dd887765197511c2b17e28ee4fd0be0c6808df3d3c6d9b1.jpg').astype('uint8')\n",
    "# get the saliency maps using the 3 implemented methods\n",
    "# plt.imshow(get_saliency_rbd(train_image_base_path + train_images[1000]).astype('uint8'))\n",
    "img_f = image.load_img(train_image_base_path + train_images[1000])\n",
    "\n",
    "target_size = (128, 128)\n",
    "img_rescaled = img_f.resize(target_size)\n",
    "img = image.img_to_array(img_rescaled).astype('uint8')\n",
    "import numpy as np\n",
    "img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "ret, thresh = cv2.threshold(img, 128, 255, 0)\n",
    "im2, contours, _ = cv2.findContours(thresh, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "largest_contour = max(contours, key=cv2.contourArea)\n",
    "mask = np.zeros(img.shape, np.uint8)\n",
    "cv2.fillPoly(mask, [largest_contour], 128)\n",
    "\n",
    "# apply mask to threshold image to remove outside. this is our new mask\n",
    "#plt.imshow(get_saliency_rbd(arr, 120).astype('uint8'))\n",
    "#kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(5, 5))\n",
    "#img = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)  # <- to remove speckles...\n",
    "#img = cv2.morphologyEx(img, cv2.MORPH_DILATE, kernel)\n",
    "#img = cv2.morphologyEx(img, cv2.MORPH_DILATE, kernel)\n",
    "#img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
    "#img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
    "#img = cv2.morphologyEx(img, cv2.MORPH_ERODE, kernel)\n",
    "#img = cv2.morphologyEx(img, cv2.MORPH_ERODE, kernel)\n",
    "plt.imshow(im2.astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grozin/kaggle/demand_prediction/saliency.py:62: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  vertices[x/num_vertices]] for x in edges]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 588 ms, sys: 0 ns, total: 588 ms\n",
      "Wall time: 586 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0ad488dcc0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAGCRJREFUeJzt3WuQHNV5BuD33dldrVZClxVCCIQRxkKygoMgW1wK4nAJmGCX5R+YMnES2aUqVRKc4MQuA0nZ8SU/oCpl7JRtUipzSxU2EGwQJShjRRaVoowFwoARErogg3WXQBckLdLevvyY1mxPMz3TM3O6e2bP+1Bb2z3T233Q7Lf9fX1On6aZQUT80pF3A0Qkewp8EQ8p8EU8pMAX8ZACX8RDCnwRDynwRTzUVOCTvJ7kJpJbSd7uqlEiki42OoCHZAHAZgDXAtgB4EUAN5vZBnfNE5E0dDbxsxcD2Gpm2wCA5MMAFgOIDfxuTrAeTGrikDWQ6e07eqhCjlVSoZDu/tMczDk6kuLOK0n5d2J0NN391+n90aMYtOM1/6ebCfwzAWwPre8AcEm1H+jBJFzCa5o4ZHWcMCG1fUd1TE7xD1gtfdNS3T2H0wtOO3I0tX1XxHT/QNuxY6nuv16/ef+pRNs1E/iJkFwGYBkA9KA37cOJSALN/DncCeCs0Pqc4LUyZrbczPrNrL8L2Z2RRSReM4H/IoB5JM8h2Q3gcwCedNMsEUlTw6m+mQ2T/BKAZwAUANxnZq87a5mIpKapGt/MngbwtKO2iEhGUr+4Jw6kfBVf/KMhuyIeUuCLeEiBL+IhBb6IhxT4Ih5S4It4SIEv4iEFvoiHFPgiHhpXI/fsxInSctJ78wunzUyrOXWzgYGxlfE6Wm/G9LxbUL8Dh/NugXM644t4SIEv4qFxleonFU7vFzyxq+y9hb27opvXbfPx05veB3Ag9p2rTtnY9N4PjWQ3G1IHG5uXrodDjluSzKYTs8vWn959fmm594vDZe9xQnfTxxs9cLDpfdRLZ3wRDynwRTykwBfxkDc1flxd76KmjzqvZ0+i7ZJeC3BR00dNK4x1HaZR7zda17eC+RN2l60/jfNjtnSjY8op7nY2mOxcrjO+iIcU+CIeGrepfq5PukkoWhK46QbMRzun9rX809xVpeW777+27L1o91670BlfxEMKfBEPKfBFPJRpjc/OAgrT+rI8pGRk1MrPIS5q/uPWVVrOa/jueFXzjE/yPpL7SK4PvdZHchXJLcH3NrzXUsRfSVL9BwBcH3ntdgCrzWwegNXBuoi0iZqpvpn9H8m5kZcXA7gyWH4QwLMAbnPYroawN34EWvQuvFaQZ/ddlnfnjWcD94+FUDt17TV6cW+WmZ0c17gHwCxH7RGRDDR9Vd/MDIDFvU9yGcl1JNcNjh5v9nAi4kCjV/X3kpxtZrtJzgawL25DM1sOYDkATO2aGfsHIuyjvzzUYLPq/7kNA2eUradx005Y0vT+5mlrS8t7Rqak1Ryp0w2z15ethyfpCKf91fR8Nb7M6tgbPwGLS42e8Z8EsCRYXgJghZvmiEgWknTn/RTA8wDmk9xBcimAOwFcS3ILgD8P1kWkTSS5qn9zzFvXOG6LiGSkZe7O2/GFBaXlj+I3ubUjWvOHua7/w3V81tKeiMOFuV3vlJb3DE/NrR3RyTfDojV/Ek//R/zEHj1fHRvZmma9r7H6Ih5S4It4qGVS/fHqrK53y9b/pOftuvdxeuG92PdapavPxU054dTelWppel6qdQlmRWd8EQ8p8EU8pMAX8ZBq/CrSHr7rQrT+7+1o4A6xrvhuox4mGmVd1Z6RZI8sd6EVa/qopDX9yOxT6963HU4W0jrji3hIgS/ioWxT/UIB6JtWXD4Qfyfdn07ZnFGDPqiXJ3I7dqtwkd6HnV4Y+zdtNO1POrpwVufhRNvtbXAkoIuut2p351lHNudinfFFPKTAF/FQflf1T6b8FXzjv/6mbP3bf/vfabdGUpTlVf2koiXB/dsvj902nJr3pNCWrNL7MJ3xRTykwBfxkAJfxEMsTpKbjak9p9tlZy+pud2OT2c333znn71beyOHbl/wi9j3zuuKnbM0VkMj9erQKiP3hqxQWr5t841N76+aA89l+7yDDz2VrAsyid+8sRzvHdvFWtvpjC/iIQW+iIda8iadOU/uKVvf/M0U51s7Uj6KasopAzEbpm/z0GlO97doQvM3GR23sayxnrR/+3D98/ilncJXMzAYejLvxeXl3/EXZmTdnNTpjC/iIQW+iIcU+CIeaskaP0/vhWr+NOr9O9+4vrRcrWsvqQ91xk+icWAkjQGmY/4w3Fd7oxr+c1s+z2UJ1/S1hGv+VOr9Tpfn35o9eQCSPULrLJJrSG4g+TrJW4PX+0iuIrkl+D69yRaLSEaS/KkZBvAVM1sI4FIAt5BcCOB2AKvNbB6A1cG6iLSBJM/O2w1gd7B8hORGAGcCWAzgymCzBwE8C+C2VFqZobS781yk92HRdLta6u/6WC7cdd5jse+l2b3X2z1Utl5P6j8e1FVckJwL4EIAawHMCv4oAMAeALOctkxEUpM48ElOBvAzAF82s7KpXa044L/i6A6Sy0iuI7lucOT9phorIm4kCnySXSgG/UNm9vPg5b0kZwfvzwZQ8Q4TM1tuZv1m1t9dmOiizSLSpJo1PkkCuBfARjP7buitJwEsAXBn8H1FKi1sc65r+lrCdXia9X49zuw82NDPVav/G+HimsF4Gc6bpB//cgB/DeA1kq8Er/0LigH/KMmlAN4GcFM6TRQR15Jc1X8O8aMC8hl9ISJN0cg98Ua4dHDVVZj6qL6UaKy+iIcU+CIeUqof4fomnfBNOUD1q/wzCkcT7fPdkcml5bSv3LsYrbdzOP42jkau+P96YF7seyt2XJBoH2mM1Ou4KNnceaO/TXFimYR0xhfxkAJfxEMKfBEPZV/jZziPfyMWfWh7qvtPWscn3ccx647dbhIHmz5W2sL1/9uDpza9v8VzXk203U+39Td9LAAYHK4/hD5wLeAZPTtPRDKgwBfxkPfdeWmn9nkKlwH1pP1pTLgxXjSS2tfSMeCuJOPoaLJjOjuiiLQNBb6IhxT4Ih5qixq/0DmSdxOcuWfXVaXlvztjTWbHjXb7XdR9JHbb+V27K77ewWRztlfzzED5I6hddOFJ/XTGF/GQAl/EQ22R6ktjqqXzeflEb/kj0BFaX37o/FSPfUnvm2PL579ZZct4f//K5101J1c644t4SIEv4iHvU/3Xdp1Rtv6xM3alejzXV/LTSOendIw9ZbfA7M4Ny6atT7RdtZIgnM6n4UeLHop9r53KAJ3xRTykwBfxkAJfxEPe1/hpSHtEXrW6PlyfuzBiY3d7pV3vT+2If7bi4dGxB65GrwUMWbI70jYMTWqsYQmF6/9Wr/drfpIke0i+QPJVkq+T/Fbw+jkk15LcSvIRkvFTwYhIS0nyJ/wEgKvN7AIAiwBcT/JSAHcBuNvMPgLgIICl6TVTRFxK8uw8A3Bykreu4MsAXA3gL4PXHwTwTQD3uG9i6/vu2U8k3nbLULI51T/eU21yhgmlpY7Yxxq2h3AKn6W1A+fGvpd2l2ArSFS0kSwET8rdB2AVgDcBHDKz4WCTHQDOTKeJIuJaosA3sxEzWwRgDoCLASxIegCSy0iuI7lucCSfv+4iUq6uy7RmdgjAGgCXAZhG8mSpMAfAzpifWW5m/WbW312Iv2orItmpWeOTnAlgyMwOkZwI4FoUL+ytAXAjgIcBLAGwwlmrOvIbXrDglL2x7y3rez7RPmYVJlR573iifVTroOoI/b0eRbLnFKR9LSCvWr1Radfxi06veB6s6ABmpdiSypL0488G8CDJAooZwqNmtpLkBgAPk/x3AC8DuDfFdoqIQ0mu6v8OwIUVXt+GYr0vIm0m45F7zDWNr+Sz818uW0+azudptEoh0BFz2SZpSfDB/Y2VCCOREXJH7URpeTLjy5vUhaqYg1VKjoVdx0rLaYziW77nSuf7TEtrRaGIZEKBL+IhL2/Siab37SwutXclXCJEewZyTe9jTI/c6FMt9Y/z3NH5rprTsnTGF/GQAl/EQwp8EQ+1RY1/7tfHumHe/E66kyk0oiuyfmBkrJurr8oovkalXddnKWkNHq3dm93f8dHop9a8hZPHHj224ehs5/t3afz8BolIYgp8EQ+1Raof1t09XHujChaf85rTdiRNFMNpP+Am9a92w02jI/SSGEb5U4s7UUi8bZxTOpLN2NZIt5zE0xlfxEMKfBEPKfBFPNQWNf72u7IbGrr8wGWx792S05171WrpqHD9X224bVjyyTzKzxNJ63gXwtcCRqz56xhX9BwrW3/ueOt1E6dJZ3wRDynwRTyUbao/OgIeOVbxLTsl3VRrxe8/Vlp23bVXTRoj95Jq9zn3w1yk90n1924rW1838GGn+x+18s9ldKK7UYTWkewz1xlfxEMKfBEPtcxV/XAJkGXaH5VlGdDuyqf5TvbEWl9E0/lWozO+iIcU+CIeUuCLeKhlavxWlNdIPcleeCRfdBRfuHvvgb1XZNamNCU+4wePyn6Z5Mpg/RySa0luJfkIyWT3V4pI7upJ9W8FsDG0fheAu83sIwAOAljqsmEikp5EgU9yDoBPAvhxsE4AVwN4LNjkQQCfcdUoHjlW9uWjThRKX6Owsi9fjJiVvsStpGf87wH4Gsae3jwDwCEzOzkdzg4AZzpum4ikpGbgk/wUgH1m9lIjByC5jOQ6kusGNX2SSEtIclX/cgCfJnkDgB4AUwB8H8A0kp3BWX8OgJ2VftjMlgNYDgBTu09TzibSAmqe8c3sDjObY2ZzAXwOwK/M7PMA1gC4MdhsCYAVqbXSQ9Vqehf1vutrBh2R/1wokKWvLF3Rc6zsazxq5hO6DcA/k9yKYs1/r5smiUja6hrAY2bPAng2WN4G4GL3TRKRtLXkyL0//Kgvs2NF78YLj9bbP9rYP8/8ruRz5DUrmqrHTb7R7t2A1dL9tLv79o9MKS3//gflj9A+50ub6t5fdB9TcbSxhjVBY/VFPKTAF/FQS6b6kyYMxr537ETztwSE9x+9EafR9D5s09DYtNP1pP37Q4/bmlVI9nTYKBcp/TsjlcdbnFpHm7KcpCPLq/7f+k75Nex/+/rYSPVqaX80vQ87PG9yaXnq5iNNtC45nfFFPKTAF/GQAl/EQy1Z41dTrf5P6rzp+0rLLmr6avZHHpM9M8d59uPE1fS1tuvtSHb9oodt92sWK9y1F1Wtjq8mq7o+TGd8EQ8p8EU8NH5ysDYRTf3j7A2l1Y127VWTNL0X4MbJe0rLjx09vey9ifuHSsvvz3T3KKy06Ywv4iEFvoiHFPgiHsr4MdkGez9JbTm59iaSuaTdd1HHS1Mzjq+uPQBY+sMnSss/+OZnc2xJfXTGF/GQAl/EQ+Mr72rAG4OzytYXdO91uv8DkZGBfR3DMVvG21ul661aV5+67LLVTl17OuOLeEiBL+Ih71P9qHDq7zrtBz6Y+oc1UgYonZdG6Iwv4iEFvoiHFPgiHvKyxt988LTS8kpcUPbep2a8WlpOu6uvUdnN2t/4aL044VF8QOuM5IveddescNceUN69F31vcHpPabn74HGn7YiT6F+d5FsAjgAYATBsZv0k+wA8AmAugLcA3GRmB9Nppoi4VE+qf5WZLTKz/mD9dgCrzWwegNXBuoi0AVqCxw8FZ/x+M3sn9NomAFea2W6SswE8a2ZVJx2b2jnTLpuyuO5GHvjJzLp/pprwnHvVhNP+KBdpfyPdd0C2qX6Y67Q/Kmna//zx8nkLdw5PT6M5Fd17y2dS3X/SVL+w692Kr/96/yM4PLiv5oMGkp7xDcAvSb5Eclnw2iwz2x0s7wEwq/KPikirSXpl5Qoz20nyNACrSL4RftPMjGTF1CH4Q7EMAHo6JjXVWBFxI9EZ38x2Bt/3AXgcxcdj7w1SfATfK+bPZrbczPrNrL+b7ueOE5H61Tzjk5wEoMPMjgTL1wH4NoAnASwBcGfwfYWrRo2eO6ds/bzp213tui1tGIyvYT/WnV1HysDoSNl6uOZ/+UR+2dyZnfH/Br99f27T+1/55aua3kdScbW7a0lS/VkAHmfxwYSdAH5iZr8g+SKAR0kuBfA2gJvSa6aIuFQz8M1sGxAZ5VJ8/V0A16TRKBFJV6LuPFeqdeeF0/vrHny+4jb1mNOdTcpUr2kdA6nuP83Uf+2J5KPb0vz/7KD7x24fH3U7ccb642eVrVcrFya+vtPZcV1354nIOKLAF/GQAl/EQ61xa1QKdgzOiH2vVet/F16L6fobsGwfz31otLe03Gi9n0Ytn5XzeyJd0N9bU1rMsnswjs74Ih5S4It4aNym+iKtauKW+LtDbXCwuZ0n7J7XGV/EQwp8EQ/llupHb8QR8VXT6X0DdMYX8ZACX8RDCnwRD2Va49vICEbeOwoAqHn7kIikRmd8EQ8p8EU8lOlEHFPYZ5cUrqvckAsXxP6ci4k54kRv2PnxF+LnTf+r+55KrR1peOBL8c8w+Md7HnF6LNcTWaRh11B28+9Hrf7kH8W+Z0ePOTvO84d+jsND+zURh4h8kAJfxEMKfBEPtUyNH1at3ndhdEJr3pQ43JtPuya+sSeX435AZ15PBYxIOSZc1vRRqvFFJJYCX8RDLZnz2stv1N4IQEd3Y11IeSaU7O6OfS+vDyOa2NpwY4/vjsVk4zTZ21t7o7R0ND+W9N1PnNv0PmY8tanpfSSR6IxPchrJx0i+QXIjyctI9pFcRXJL8D2/TlIRqUvSVP/7AH5hZgtQfJzWRgC3A1htZvMArA7WRaQNJHla7lQAHwfwBQAws0EAgyQXA7gy2OxBAM8CuC2NRoq0Khfpfdn+Pjk/9j2XZUCSM/45APYDuJ/kyyR/HDwue5aZ7Q622YPiU3VFpA0kCfxOABcBuMfMLgRwDJG03oqDASp2fpJcRnIdyXVDONFse0XEgSSBvwPADjNbG6w/huIfgr0kZwNA8L3inMFmttzM+s2svwvZPs1FRCqrWeOb2R6S20nON7NNAK4BsCH4WgLgzuD7ilRbKuOeDZQ/aivX7r2EZjzzZmnZdb0fZYNDtTcaTTbqMGnX8T8AeIhkN4BtAL6IYrbwKMmlAN4GcFPCfYlIzhIFvpm9AqC/wlvXuG2OiGShJUfuiQDlqb/ztN/BSL2oRtP+vsfXO29LLRqrL+IhBb6IhxT4Ih5SjS9tIdrVF5a4/k+hro8TrvcBwAbez+zYSeiML+IhBb6IhzKdc4/kfhQH+5wK4J3MDlxZK7QBUDui1I5y9bbjbDObWWujTAO/dFBynZlVGhDkVRvUDrUjr3Yo1RfxkAJfxEN5Bf7ynI4b1gptANSOKLWjXCrtyKXGF5F8KdUX8VCmgU/yepKbSG4lmdmsvCTvI7mP5PrQa5lPD07yLJJrSG4g+TrJW/NoC8keki+QfDVox7eC188huTb4fB4J5l9IHclCMJ/jyrzaQfItkq+RfIXkuuC1PH5HMpnKPrPAJ1kA8EMAfwFgIYCbSS7M6PAPALg+8loe04MPA/iKmS0EcCmAW4J/g6zbcgLA1WZ2AYBFAK4neSmAuwDcbWYfAXAQwNKU23HSrShO2X5SXu24yswWhbrP8vgdyWYqezPL5AvAZQCeCa3fAeCODI8/F8D60PomALOD5dkANmXVllAbVgC4Ns+2AOgF8FsAl6A4UKSz0ueV4vHnBL/MVwNYCYA5teMtAKdGXsv0cwEwFcDvEVx7S7MdWab6ZwLYHlrfEbyWl1ynByc5F8CFANbm0ZYgvX4FxUlSVwF4E8AhMzv5/KysPp/vAfgagNFgfUZO7TAAvyT5EsllwWtZfy6ZTWWvi3uoPj14GkhOBvAzAF82s/fyaIuZjZjZIhTPuBcDSPfZ5BWQ/BSAfWb2UtbHruAKM7sIxVL0FpIfD7+Z0efS1FT29cgy8HcCOCu0Pid4LS+Jpgd3jWQXikH/kJn9PM+2AICZHQKwBsWUehrJk7dqZ/H5XA7g0yTfAvAwiun+93NoB8xsZ/B9H4DHUfxjmPXn0tRU9vXIMvBfBDAvuGLbDeBzAJ7M8PhRT6I4LTiQ0fTgJAngXgAbzey7ebWF5EyS04LliSheZ9iI4h+AG7Nqh5ndYWZzzGwuir8PvzKzz2fdDpKTSJ5ychnAdQDWI+PPxcz2ANhO8uRztE5OZe++HWlfNIlcpLgBwGYU68l/zfC4PwWwG8AQin9Vl6JYS64GsAXA/wLoy6AdV6CYpv0OwCvB1w1ZtwXAHwN4OWjHegDfCF7/MIAXAGwF8D8AJmT4GV0JYGUe7QiO92rw9frJ382cfkcWAVgXfDZPAJieRjs0ck/EQ7q4J+IhBb6IhxT4Ih5S4It4SIEv4iEFvoiHFPgiHlLgi3jo/wFPYr6B8wwAigAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%time plt.imshow(get_saliency_rbd(arr, 100, 20).astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = train_image_base_path + train_images[0]\n",
    "\n",
    "img_f = image.load_img(filename)\n",
    "img = image.img_to_array(img_f)\n",
    "\n",
    "target_size = (224, 224)\n",
    "img_rescaled = img_f.resize(target_size)\n",
    "rgb_array = image.img_to_array(img_rescaled)\n",
    "\n",
    "feat = cv2.FastFeatureDetector_create()\n",
    "\n",
    "kp = feat.detect(img, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking validity, processing leftovers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_class_ids = []\n",
    "for i in range(8):\n",
    "    with open(str(i) + \"_img_classes.pickle\", \"rb\") as f:\n",
    "        while True:\n",
    "            try:\n",
    "                pair = pickle.load(f)\n",
    "                processed_class_ids.append(pair[0])\n",
    "            except:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_feature_ids = []\n",
    "for i in range(8):\n",
    "    with open(str(i) + \"_features.pickle\", \"rb\") as f:\n",
    "        while True:\n",
    "            try:\n",
    "                pair = pickle.load(f)\n",
    "                processed_feature_ids.append(pair[0])\n",
    "            except:\n",
    "                break\n",
    "    with open(str(i) + \"_features_test.pickle\", \"rb\") as f:\n",
    "        while True:\n",
    "            try:\n",
    "                pair = pickle.load(f)\n",
    "                processed_feature_ids.append(pair[0])\n",
    "            except:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_class_ids = set(processed_class_ids)\n",
    "processed_feature_ids = set(processed_feature_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sum(~training[\"image\"][~training[\"image\"].isnull()].apply(lambda x: x + \".jpg\" in processed_class_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sum(~testing[\"image\"][~testing[\"image\"].isnull()].apply(lambda x: x + \".jpg\" in processed_class_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from TernausNet.unet_models import unet11\n",
    "from pathlib import Path\n",
    "from torch.nn import functional as F\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = unet11(pretrained='carvana')\n",
    "    model.eval()\n",
    "    return model.to(device)\n",
    "\n",
    "def mask_overlay(image, mask, color=(0, 255, 0)):\n",
    "    \"\"\"\n",
    "    Helper function to visualize mask on the top of the car\n",
    "    \"\"\"\n",
    "    mask = np.dstack((mask, mask, mask)) * np.array(color)\n",
    "    mask = mask.astype(np.uint8)\n",
    "    weighted_sum = cv2.addWeighted(mask, 0.5, image, 0.5, 0.)\n",
    "    img = image.copy()\n",
    "    ind = mask[:, :, 1] > 0    \n",
    "    img[ind] = weighted_sum[ind]    \n",
    "    return img\n",
    "\n",
    "def load_image(path, pad=True):\n",
    "    \"\"\"\n",
    "    Load image from a given path and pad it on the sides, so that eash side is divisible by 32 (newtwork requirement)\n",
    "    \n",
    "    if pad = True:\n",
    "        returns image as numpy.array, tuple with padding in pixels as(x_min_pad, y_min_pad, x_max_pad, y_max_pad)\n",
    "    else:\n",
    "        returns image as numpy.array\n",
    "    \"\"\"\n",
    "    img = cv2.imread(str(path))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    if not pad:\n",
    "        return img\n",
    "    \n",
    "    height, width, _ = img.shape\n",
    "    \n",
    "    if height % 32 == 0:\n",
    "        y_min_pad = 0\n",
    "        y_max_pad = 0\n",
    "    else:\n",
    "        y_pad = 32 - height % 32\n",
    "        y_min_pad = int(y_pad / 2)\n",
    "        y_max_pad = y_pad - y_min_pad\n",
    "        \n",
    "    if width % 32 == 0:\n",
    "        x_min_pad = 0\n",
    "        x_max_pad = 0\n",
    "    else:\n",
    "        x_pad = 32 - width % 32\n",
    "        x_min_pad = int(x_pad / 2)\n",
    "        x_max_pad = x_pad - x_min_pad\n",
    "    \n",
    "    img = cv2.copyMakeBorder(img, y_min_pad, y_max_pad, x_min_pad, x_max_pad, cv2.BORDER_REFLECT_101)\n",
    "\n",
    "    return img, (x_min_pad, y_min_pad, x_max_pad, y_max_pad)\n",
    "\n",
    "img_transform = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def crop_image(img, pads):\n",
    "    \"\"\"\n",
    "    img: numpy array of the shape (height, width)\n",
    "    pads: (x_min_pad, y_min_pad, x_max_pad, y_max_pad)\n",
    "    \n",
    "    @return padded image\n",
    "    \"\"\"\n",
    "    (x_min_pad, y_min_pad, x_max_pad, y_max_pad) = pads\n",
    "    height, width = img.shape[:2] \n",
    "    \n",
    "    return img[y_min_pad:height - y_max_pad, x_min_pad:width - x_max_pad]\n",
    "\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "img, pads = load_image(train_image_base_path + train_images[101], pad=True)\n",
    "with torch.no_grad():\n",
    "    input_img = torch.unsqueeze(img_transform(img).to(device), dim=0)\n",
    "with torch.no_grad():\n",
    "    mask = F.sigmoid(model(input_img))\n",
    "mask_array = mask.data[0].cpu().numpy()[0]\n",
    "mask_array = crop_image(mask_array, pads)\n",
    "mask_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "training = pd.read_csv(base_path + 'train.csv', index_col = \"item_id\", parse_dates = [\"activation_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "training = training[(training[\"image\"].fillna(\"\") + \".jpg\").isin(train_features.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_count = len(simple_img_features_names)\n",
    "zero_obj = -np.ones((col_count))\n",
    "\n",
    "packed_images_vectors = training[\"image\"].apply(lambda x: train_features.get(str(x) + \".jpg\", zero_obj))\n",
    "training[simple_img_features_names] = pd.DataFrame(packed_images_vectors.values.tolist(), index=training.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = [\"region\", \"city\",\n",
    "               \"parent_category_name\", \"category_name\", \n",
    "               \"user_type\",\n",
    "               \"image_top_1\",\n",
    "               \"param_1\", \"param_2\", \"param_3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = training[\"deal_probability\"]\n",
    "training = training[categorical + simple_img_features_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "for col in categorical:\n",
    "    training[col].fillna('Unknown')\n",
    "    training[col] = lbl.fit_transform(training[col].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_params =  {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'num_leaves': 270,\n",
    "    'feature_fraction': 0.5,\n",
    "    'bagging_fraction': 0.75,\n",
    "    'bagging_freq': 2,\n",
    "    'learning_rate': 0.05,\n",
    "    'verbose': 0,\n",
    "    \"num_threads\": 8,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training[categorical]\n",
    "X_train, X_test, y_test, y_test = train_test_split(np.array(X), target, test_size=0.10, random_state=23)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(np.array(X), target, test_size=0.10, random_state=23)\n",
    "\n",
    "lgtrain = lgb.Dataset(X_train, y_train,\n",
    "                feature_name=categorical,\n",
    "                categorical_feature = categorical)\n",
    "lgvalid = lgb.Dataset(X_valid, y_valid,\n",
    "                feature_name=categorical,\n",
    "                categorical_feature = categorical)\n",
    "lgtest = lgb.Dataset(X_test, y_test,\n",
    "                feature_name=categorical,\n",
    "                categorical_feature = categorical)\n",
    "\n",
    "lgb_model = lgb.train(lgbm_params, lgtrain, 200, valid_sets=(lgvalid), valid_names=['val'], verbose_eval=10, early_stopping = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(lgb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(lgb_model.predict(X_test, lgb_model.best_iteration), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = training[categorical + simple_img_features_names]\n",
    "X_train, X_test, y_test, y_test = train_test_split(np.array(X), target, test_size=0.10, random_state=23)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(np.array(X), target, test_size=0.10, random_state=23)\n",
    "\n",
    "lgtrain = lgb.Dataset(X_train, y_train,\n",
    "                feature_name=categorical + simple_img_features_names,\n",
    "                categorical_feature = categorical)\n",
    "lgvalid = lgb.Dataset(X_valid, y_valid,\n",
    "                feature_name=categorical + simple_img_features_names,\n",
    "                categorical_feature = categorical)\n",
    "lgtest = lgb.Dataset(X_test, y_test,\n",
    "                feature_name=categorical + simple_img_features_names,\n",
    "                categorical_feature = categorical)\n",
    "\n",
    "lgb_model = lgb.train(lgbm_params, lgtrain, 200, valid_sets=(lgvalid), valid_names=['val'],\n",
    "                      verbose_eval=10, early_stopping_rounds= 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lgb.plot_importance(lgb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(lgb_model.predict(X_test, lgb_model.best_iteration), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
